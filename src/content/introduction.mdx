import Block from '../components/Block.astro';
import Figure from '../components/Figure.astro';
import FigureEnv from '../components/FigureEnv.astro';
import HStack from '../components/HStack.astro';

export const BASE_URL = import.meta.env.BASE_URL;

Imitation Learning (IL), using teleoprated or synthetic demonstration data, has become a cornerstone of general-purpose robotics platforms. Most IL pipelines have converged on three defining pipeline characteristics:

  1. The prediction of open-loop sequences, or "chunks" of actions by the control policy, called **action-chunking** (AC),
  2. The careful curation of expert data to be imitated.
  3. The adoption of **generative** neural architectures (e.g. conditional diffusion models) as parameterizations of learned policies. 
  
While the benefits of (3) have been studied broadly, a precise understanding of how action-chunking and curated expert data improve behavior cloning performance remains elusive.

We provide the first substantive theoretical guarantees justifying the practices of AC and exploratory data augmentation during expert data collection in the **minimal** setting of imitation of an expert in a state-based continuous-control problem. Our point of departure is the finding in recent work that imitation learning in continuous settings&mdash;even those whose dynamics and expert demonstrator appear benign&mdash;can be considerably more challenging than imitation in discrete settings, such as those encountered in language modeling, demonstrating compounding errors can grow exponentially with horizon, as opposed to polynomially (or none).


Concretely, we study the following widespread practices:

**Intervention 1: Action-Chunking.** When the environment is benign, we show the algorithmic modification of **action-chunking**, i.e., predicting and playing open-loop sequences of actions, mitigates compounding errors without requiring any modification to the expert data.

**Intervention 2: Exploratory Data Collection via Expert Noise-Injection.** When the environment is less benign, some alteration of the expert data distribution is necessary. We demonstrate **noise-injection**, i.e., adding noise while executing expert actions, is a simple and practical tool for avoiding compounding errors.

<FigureEnv>
  <HStack>
    <Figure src={`${BASE_URL}/figs/action_chunking.svg`} alt="Overview of Action Chunking and Noise Injection" height="25rem"/>
  </HStack>
</FigureEnv>

<Block type="Surprising Takeaways">
While our interventions are reflective of popular practices at the intersection of (reinforcement) learning and control, our analysis additionally uncovers phenomena that contrast with the common perspectives of both literatures.

1. **Rationale for action-chunking:** Action-chunking has been motivated by both enabling larger policy latency, and encouraging long-horizon planning and stronger policy representations. We illuminate an orthogonal rationale: **action-chunking encourages control-theoretic stability of policies learned**, mitigating possibly exponential compounding errors from unstable closed-loop interactions.

2. **Sufficiency of Non-Interactive Exploration:** Various prior works have approached the compounding errors problem by adaptively querying the expert policy at possibly suboptimal states, with the overarching goal of **witnessing how the expert policy recovers from errors**. We expose the key directions of (additional) supervision around expert trajectories required to achieve stable imitation, and **we attain this supervision through noising expert actions at a fixed, isotropic scale**. In doing so, we establish that&mdash;unlike what an online learning perspective may suggest&mdash;**iterative, adaptive interaction with an expert is unnecessary to achieve near-optimal expert imitation**.

3. **Towards New Notions of "Coverage" in Continuous State Space:** The lower bounds show that standard notions of "coverage" in theoretical reinforcement learning (which is **maximal** when learning from expert data) is insufficient for mitigating compounding error in continuous behavior cloning. Our work reveals that **novel, stronger notions of coverage realized via noise injection do suffice in continuous state spaces**, and lead to guarantees that are sharper than coverage-based arguments which leverage exploratory data.

4. **Towards New Notions of "Excitation" in Continuous State Space:** Despite requiring stronger coverage, our bounds do not require the injected noise to produce "persistent excitation" from control theory, i.e. uniform variation across all state directions. Rather **naive exploration** via white noise suffices, even when the underlying system is not controllable. This is because the **error-directions which are susceptible to compounding error are precisely those along which noise injection provides supervision**.
</Block>

