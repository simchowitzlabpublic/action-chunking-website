import Block from '../components/Block.astro';
import Figure from '../components/Figure.astro';
import FigureEnv from '../components/FigureEnv.astro';
import HStack from '../components/HStack.astro';

export const BASE_URL = import.meta.env.BASE_URL.replace(/\/+$/, '');

## Preliminaries

### Control Systems 
To isolate the effects of compounding error, we consider the minimal setting of imitation learning of a **deterministic, Markovian expert** in a fully observed, continuous state-action environment. Speficially, we adopt the language of a **control system** as a natural abstraction for decision making in robot learning.

A discrete-time, continuous state-action control system is defined by 

1. **States** $\mathbf{x}_t \in \mathcal{X} = \mathbb{R}^{d_x}$
2. **Control inputs** $\mathbf{u}_t \in \mathcal{U} = \mathbb{R}^{d_u}$, which correspond to actions.
3. **Dynamics** deterministically evolving according to $\mathbf{x}_{t+1} = f(\mathbf{x}_t,\mathbf{u}_t)$.

Thus, a control system is just an Markov Decision Process (MDP) with deterministic transitions and no rewards, tailored to continuous state and action spaces. We assume the initial state is drawn $\mathbf{x}_1 \sim D$ for some distribution $D$ fixed throughout.


### Imitation Learning in Control Systems 

**TODO** setup the imitation learning problem. 


A deterministic policy $\pi$ maps histories of states, inputs, and the current time step to a control input $\mathbf{u}_t = \pi(\mathbf{x}_{1:t},\mathbf{u}_{1:t-1},t)$.  Given two deterministic policies $\pi_1,\pi_2$, we let $\mathbb{E}_{\pi_1,\pi_2}$ denote the expectation of sequences $(\mathbf{x}_t^{\pi_i},\mathbf{u}_t^{\pi_i})_{t \ge 1}$ under the dynamics $f$, coupled so that $\mathbf{x}_1^{\pi_1} = \mathbf{x}_1^{\pi_2} \sim D$. We consider estimation of **deterministic, Markovian** expert policies $\pi^*: \mathcal{X} \to \mathcal{U}$ given a problem horizon $T$. Our aim is to learn some policy $\hat{\pi}$ which accumulates low squared-**trajectory error**:

$$
J_{\mathrm{imitation}}(\hat{\pi}) \triangleq \mathbb{E}_{\hat{\pi},\pi^*}\left[\sum_{t=1}^T \min\left\{1,\|\hat{\mathbf{x}}_t - \mathbf{x}_t^*\|^2 +\|\hat{\mathbf{u}}_t - \mathbf{u}_t^* \|^2\right\}\right].
$$


### The Compounding Errors Problem


We say $\pi$ is Markovian and time-invariant if we can simply express $\mathbf{u}_t = \pi(\mathbf{x}_t)$. In this case, we define the closed-loop dynamics $f^{\pi}(\mathbf{x},\mathbf{u}) \triangleq f(\mathbf{x}, \pi(\mathbf{x}) + \mathbf{u})$, and $f^{\pi}(\mathbf{x}) = f^{\pi}(\mathbf{x},0)$.

<Block title="Compounding Error" type="Definition">
Let $\mathcal{A}$ be a (possibly randomized) mapping from a sample of $n$ trajectories $S_n \sim \mathcal{P}_{\text{demo}}$ to an imitator policy $\hat{\pi} \sim \text{alg}(S_n)$. The problem instance suffers **exponential compounding errors** if:

<br/>
$$E_{\hat{\pi}, S_n}[J_{\mathrm{imitation}}(\hat{\pi})] \gtrsim C^T \cdot E_{\hat{\pi}, S_n}[J_{\mathrm{demo}}(\hat{\pi}; \mathcal{P}_{\text{demo}})],$$

for some $C > 1$.  **TODO fix spacing**
</Block>
In other words, imitating via empirical risk minimization on a given demonstration distribution $\mathcal{P}_{\text{demo}}$ leads to learned policies $\hat{\pi}$ that suffer exponentially more **trajectory error** rolled out in closed-loop compared to their **on-expert** regression error.


### "Nice" Control Systems: Incremental Stability



As proposed in prior work, compounding error can be understood through the lens of control-theoretic **stability**, which describes the sensitivity of the dynamics to perturbations of the state or input. We consider a notion of **incremental stability**.

<Block title="EISS" type="Definition">
A system $\mathbf{x}_{t+1} = f(\mathbf{x}_t, \mathbf{u}_t)$ is $(C_{\text{ISS}}, \rho)$-**exponentially incrementally input-to-state stable** (EISS) if for all pairs of initial conditions $(\mathbf{x}_1,\mathbf{x}_1')$ and input sequences $(\{\mathbf{u}_t\}_{t\geq1}, \{\mathbf{u}_t'\}_{t\geq1})$, there exist constants $C_{\text{ISS}} \geq 1$, $\rho \in (0, 1)$ such that for any $t \geq 1$:
$$
\|\mathbf{x}_t - \mathbf{x}_t'\| \leq C_{\text{ISS}} \rho^{t-1} \|\mathbf{x}_1 - \mathbf{x}_1'\| + C_{\text{ISS}} \sum_{k=1}^{t-1} \rho^{t-1-k} \|\mathbf{u}_k - \mathbf{u}_k'\|, \quad t \geq 1.
$$
We say a policy-dynamics pair $(\pi,f)$ is $(C_{\text{ISS}},\rho)$-EISS if the induced closed-loop dynamics $f^\pi$ is $(C_{\text{ISS}},\rho)$-EISS.
</Block>


<FigureEnv>
  <HStack>
    <Figure src={`${BASE_URL}/figs/stability_diagram.svg`}
        alt="Visualization of EISS (Exponentially Incrementally Input-to-State Stable)" />
  </HStack>
</FigureEnv>

<Block title="" type="Is EISS a reasonable assumption for some robotic manipulation settings? Yes">
 Many robotic manipulation settings involve quasi-static interactions between the robot and objects, where the robot controls its end-effector directly, and the objects move in response to contact forces. In these settings, the dynamics from end-effector commands to object states are often stable, as small perturbations in end-effector position lead to small perturbations in object position, without amplification over time. 
</Block>
**TODO: Insert tracking figure

### Exponential Stability *Does Not* Prevent Exponential Compounding Errors

**Explain this point** 