import Block from '../components/Block.astro';
import Figure from '../components/Figure.astro';
import FigureEnv from '../components/FigureEnv.astro';
import HStack from '../components/HStack.astro';

export const BASE_URL = import.meta.env.BASE_URL.replace(/\/+$/, '');


## Action-Chunking *Does* Prevent Exponential Compounding Errors  in Open-Loop Stable Systems

Action-chunking is a popular practice in modern sequential modeling pipelines, where a policy predicts a sequence of actions, of which some number are played **in open-loop**. There are various intuitions of the practical benefits of action-chunking, ranging from:

 1. Robustness to non-Markovian / partial observability quirks in the data.
 2. Amenability to multi-modal prediction.
 3. Improved representation learning via multi-step prediction.
 4. Simulating Model-Predictive Control.


<Block title="Chunking Policy" type="Definition">
A chunking policy is specified by a chunk-length $\ell$, and a chunking policy $\text{chunk}[\pi]: \mathcal{X} \to \mathcal{U}^{\ell}$ such that $\pi(\mathbf{x}_{1:t},\mathbf{u}_{1:t-1},t) = \text{chunk}[\pi](\mathbf{x}_{\ell k})_{t - \ell k}$ where $k = \lfloor \frac{t}{\ell}\rfloor$, i.e. we predict $\ell$-length sequences which are then executed "open-loop" without feedback from $\mathbf{x}$ until the chunk has been exhausted.
</Block>
For convenience we also write $\text{chunk}[\pi](\mathbf{x}) = (\text{chunk}_1(\mathbf{x}),\dots, \text{chunk}_{\ell}(\mathbf{x}))$ and denote a chunking policy as $\hat{\pi}_{\text{chunk}}$. For chunked policies, our demonstration loss becomes:

$$
J_{\text{demo}}(\hat{\pi}_{\text{chunk}}) = \mathbb{E}_{\pi^\star}\left[ \sum_{k=1}^{(T-1)/\ell} \|\mathbf{u}^*_{1+(k-1)\ell:k\ell} - \text{chunk}[\hat{\pi}_{\text{chunk}}](\mathbf{x}^*_{(k-1)\ell})\|^2\right].
$$

<Block type="Intervention 1" title="Learning over Chunked Policies">
We sample $S_n$ i.i.d. trajectories drawn from the expert distribution $\mathcal{P}_{\text{demo}}$. Instead of learning $\hat{\pi}: \mathcal{X} \to \mathcal{U}$, we learn a $\ell$-chunked-policy $\text{chunk}[\hat{\pi}_{\text{chunk}}]: \mathcal{X} \to \mathcal{U}^\ell$, that attains low **on-expert error** $J_{\text{demo}}(\hat{\pi}_{\text{chunk}})$, e.g., by empirical risk minimization.
</Block>

The Control-Theoretic intuition behind this intervention is that, by making the chunk length long enough, the learned policy $\hat{\pi}_{\text{chunk}}$ **inherits the open-loop stability of the dynamics $f$**.

<FigureEnv>
    <Figure src={`${BASE_URL}/figs/action_chunking.svg`} alt="Visualization of the stabilizing effect of action chunks"/>
    $(C_{\text{ISS}}, \rho)$-EISS can be visualized as a "funnel" which coerces the system state towards the commanded trajectories.
    By making the chunk-length sufficiently long such that $C_{\text{ISS}}\rho^{\ell} \lesssim 1$, we can guarantee that $\hat{\pi}$ is also EISS, i.e. *contractive*.
</FigureEnv>


<Block type="Key Result">
Let the true dynamics $f$ are $(C_{\text{ISS}},\rho)$-EISS, and all chunk mappings $\text{chunk}[\pi]$ in consideration are $L_\pi$-Lipschitz. For sufficiently long chunk-length:
$$
\ell > \log(1/\rho)^{-1} \cdot \log(\text{poly}(L_\pi, \hat{C}_{\text{ISS}}))
$$
Then we have the trajectory-error bound for **any $\ell$-chunking policy $\hat{\pi}_{\text{chunk}}$**,

$$
J_{\mathrm{imitation}}(\hat{\pi}_{\text{chunk}}) \leq O^*(1)J_{\text{demo}}(\hat{\pi}_{\text{chunk}}; \mathcal{P}_{\text{exp}}).
$$

This implies that when the ambient dynamics $f$ are EISS, then a sufficiently chunked imitator policy will accrue limited compounding errors&mdash;**horizon-free**&mdash;relative to the on-expert error it sees.
</Block>
